#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed May  7 17:51:15 2025

@author: mdb
"""

import os
import yaml
import subprocess
import copy
import math
import numpy as np
import shutil
from collections import defaultdict
from pathlib import Path
import xml.etree.ElementTree as ET
import csv
from utils.re_ranking import re_ranking

###########################################################################################################################
# ================== ELIJO EL BLOQUE DE MODELOS Y DATASETS A USAR ==================
USE_MODEL_PAIR = 'Large'         # opciones: 'resnet50', 'seresnet50', 'resnet101', 'seresnet50_IBN'. 'resnet_50_101 NOTA: AÑADE MODELOS FACIL ABAJO
USE_DATASET_SET = 'original_3x'   # opciones: 'original_sr_st', 'augmv1_augmv2', 'original_3x'
METRIC_LOSS_TYPE = 'triplet'  # opciones: 'triplet_center', 'quadruplet'
USE_UNIFIED_MODEL = False  # True es para unified, False para por clases NOTA: No está refinado, falta terminar  -> Meanwhile por False
###########################################################################################################################


# ================== CONFIGURACIÓN GENERAL ==================
CLASSES = ["Rubish", "Crosswalks", "Containers"]
TRANSFER_ROOT = "/home/mdb/reid-strong-baseline/TRANSFER_Learning_SR_ensembling/"
KAGGLE_ROOT = "/home/mdb/DL_Lab3/Kaggle_Dataset"
RIVAS_ROOT = "/home/mdb/DL_Lab3/RIVAS_DATASET/DATASET"

base_dir = Path(TRANSFER_ROOT)
global_query_xml = Path(KAGGLE_ROOT) / "query_label.xml"
dataset_dir = Path(RIVAS_ROOT)
results_dir = base_dir / "FINAL"


###################################### MODELOS
MODEL_CONFIGS = {
    'Large': {
        'models': ["vit_large_patch16_224_TransReID", "vit_large_patch16_224_TransReID"]
    }
}

#################################### DATASETS
DATASET_CONFIGS = {
    'original_sr_st': {
        "base": {
            "UAM": "/home/mdb/DL_Lab3/UAM_DATASET/stratified/{}/",
            "RIVAS": "/home/mdb/DL_Lab3/RIVAS_DATASET/DATASET/{}/",
        },
        "sr": {
            "UAM": "/home/mdb/DL_Lab3/ST_DATASETS/UAM_SR/{}/",
            "RIVAS": "/home/mdb/DL_Lab3/ST_DATASETS/RIVAS_SR/{}/",
        },
        "sr_st": {
            "UAM": "/home/mdb/DL_Lab3/ST_DATASETS/UAM_SR_ST/{}/",
            "RIVAS": "/home/mdb/DL_Lab3/ST_DATASETS/RIVAS_SR_ST/{}/",
        }
    },
    'augmv1_augmv2': {
        "base": {
            "UAM": "/home/mdb/DL_Lab3/UAM_DATASET/stratified/{}/",
            "RIVAS": "/home/mdb/DL_Lab3/RIVAS_DATASET/DATASET/{}/",
        },
        "augm1": {
            "UAM": "/home/mdb/DL_Lab3/ST_DATASETS/UAM_AUGM/{}/",
            "RIVAS": "/home/mdb/DL_Lab3/ST_DATASETS/RIVAS_AUGM/{}/",
        },
        "augm2": {
            "UAM": "/home/mdb/DL_Lab3/ST_DATASETS/UAM_AUGM2/{}/",
            "RIVAS": "/home/mdb/DL_Lab3/ST_DATASETS/RIVAS_AUGM2/{}/",
        }
    },
    'original_3x': {
        "base": {
            "UAM": "/home/mdb/DL_Lab3/UAM_DATASET/stratified/{}/",
            "RIVAS": "/home/mdb/DL_Lab3/RIVAS_DATASET/DATASET/{}/",
        },
        "base2": {
            "UAM": "/home/mdb/DL_Lab3/UAM_DATASET/stratified/{}/",
            "RIVAS": "/home/mdb/DL_Lab3/RIVAS_DATASET/DATASET/{}/",
        },
        "base3": {
            "UAM": "/home/mdb/DL_Lab3/UAM_DATASET/stratified/{}/",
            "RIVAS": "/home/mdb/DL_Lab3/RIVAS_DATASET/DATASET/{}/",
        }
    }
}



# ================== Rutas unificadas ==================
UNIFIED_DATASET_PATHS = {
    'original_3x': {
        "base": {
            "UAM": "/home/mdb/DL_Lab3/ST_DATASETS/UAM/Unified/",
            "RIVAS": "/home/mdb/DL_Lab3/ST_DATASETS/RIVAS/Unified/",
        },
        "base2": {
            "UAM": "/home/mdb/DL_Lab3/ST_DATASETS/UAM/Unified/",
            "RIVAS": "/home/mdb/DL_Lab3/ST_DATASETS/RIVAS/Unified/",
        },
        "base3": {
            "UAM": "/home/mdb/DL_Lab3/ST_DATASETS/UAM/Unified/",
            "RIVAS": "/home/mdb/DL_Lab3/ST_DATASETS/RIVAS/Unified/",
        }
    }
}

# ================== Modificación de DATASET_CONFIGS si se usa el modo unificado ==================
if USE_UNIFIED_MODEL:
    DATASET_CONFIGS = UNIFIED_DATASET_PATHS
    CLASSES = ["Unified"]


MODEL_NAMES = MODEL_CONFIGS[USE_MODEL_PAIR]['models']
PREWEIGHTS = MODEL_CONFIGS[USE_MODEL_PAIR]['weights']
DATASET_PAIRS = DATASET_CONFIGS[USE_DATASET_SET]

BASE_YAML_TEMPLATE = {
    'DATALOADER': {'NUM_INSTANCE': 4, 'NUM_WORKERS': 8, 'SAMPLER': 'softmax_triplet'},
    'SOLVER': {
        'OPTIMIZER_NAME': 'AdamW', 'MAX_EPOCHS': 1, 'BASE_LR': 0.001, 'WEIGHT_DECAY': 0.0005,
        'WARMUP_METHOD': 'linear', 'IMS_PER_BATCH': 64, 'LOG_PERIOD': 60, 'CHECKPOINT_PERIOD':1,
        'EVAL_PERIOD': 1, 'BIAS_LR_FACTOR': 2, 'LARGE_FC_LR': False, 'SEED': 1234,
        'WEIGHT_DECAY_BIAS': 1e-4, 'MARGIN' : 0.3, 'MARGIN2' : 0.3
    },
    'INPUT': {
        'SIZE_TRAIN': [256, 128], 'SIZE_TEST': [256, 128],
        'PIXEL_MEAN': [0.5, 0.5, 0.5], 'PIXEL_STD': [0.5, 0.5, 0.5],
        'PADDING': 10,
        'REA': {'ENABLED': True}, 'LGT': {'DO_LGT': True, 'PROB': 0.5}
    },
    'MODEL': {
        'NAME': 'part_attention_vit', 'STRIDE_SIZE': [12, 12], 'NO_MARGIN': False,
        'IF_WITH_CENTER': 'yes', 'IF_LABELSMOOTH': 'on',
        'METRIC_LOSS_TYPE': "triplet",
        'PRETRAIN_CHOICE': 'imagenet', 'PRETRAIN_PATH': '/home/mdb/DL_Lab3/Part-Aware-Transformer/config/',
        },
    'DATASETS': {'ROOT_DIR': '', 'TRAIN': ['UAM'], 'TEST': ['UAM']},
    'TEST': {'EVAL': True, 'FEAT_NORM': True, 'NECK_FEAT': 'before', 'RE_RANKING': True, 'IMS_PER_BATCH': 128},
    'LOG_ROOT': '', 'LOG_NAME': '', 'TB_LOG_ROOT': '',
    'OUTPUT_DIR': ''
}


# ============================================================================================================ Funciones auxiliares ==================

def run_training(config_path):
    subprocess.run(["python", "train.py", "--config_file", config_path],stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)

def run_update(config_path, track_path):
    subprocess.run(["python", "update.py", "--config_file", config_path, "--track", track_path], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)

def l2norm(x):
    return x / np.linalg.norm(x, axis=1, keepdims=True)

def fusionar_tracks(track_paths, output_path, topk=100, decay=0.075):
    track_preds = []
    for path in track_paths:
        with open(path, 'r') as f:
            lines = [line.strip().split() for line in f.readlines()]
            track_preds.append(lines)

    num_queries = len(track_preds[0])

    with open(output_path, 'w') as fout:
        for q_idx in range(num_queries):
            votes = defaultdict(float)
            for track in track_preds:
                preds = track[q_idx]
                for rank, pred in enumerate(preds):
                    try:
                        pred_idx = int(pred)
                        weight = math.exp(-decay * rank)
                        votes[pred_idx] += weight
                    except ValueError:
                        continue

            sorted_preds = sorted(votes.items(), key=lambda x: -x[1])
            top_preds = [str(idx) for idx, _ in sorted_preds[:topk]]
            fout.write(" ".join(top_preds) + "\n")

# ================== Entrenamiento y Update ==================
os.makedirs(TRANSFER_ROOT, exist_ok=True)

for cls in CLASSES:
    for model_type in MODEL_NAMES:
        for pair_name, paths in DATASET_PAIRS.items():
            print(f"ENTRENANDO: {cls}_{model_type}_{pair_name} ")

            class_model_root = TRANSFER_ROOT / f"{cls}_{model_type}_{pair_name}"
            uam_dir = class_model_root / "UAM"
            rivas_dir = class_model_root / "RIVAS"
            uam_dir.mkdir(parents=True, exist_ok=True)
            rivas_dir.mkdir(parents=True, exist_ok=True)

            # --- CONFIG TRAIN UAM ---
            uam_yaml = copy.deepcopy(BASE_YAML_TEMPLATE)
            uam_yaml['MODEL']['TRANSFORMER_TYPE'] = model_type
            uam_yaml['DATASETS']['ROOT_DIR'] = paths['UAM'].format(cls)
            uam_yaml['DATASETS']['TRAIN'] = ['UAM']
            uam_yaml['DATASETS']['TEST'] = ['UAM']
            uam_yaml['LOG_ROOT'] = str(uam_dir / "log")
            uam_yaml['LOG_NAME'] = str(uam_dir / "model")
            uam_yaml['TB_LOG_ROOT'] = str(uam_dir / "tb_log")
            uam_yaml['OUTPUT_DIR'] =  class_model_root / "UAM"
            if cls == "Rubish":
                uam_yaml['INPUT']['SIZE_TRAIN'] = [160, 224]
                uam_yaml['INPUT']['SIZE_TEST'] = [160, 224]
            elif cls == "Crosswalks":
                uam_yaml['INPUT']['SIZE_TRAIN'] = [320, 96]
                uam_yaml['INPUT']['SIZE_TEST'] = [320, 96]
            elif cls == "Containers":
                uam_yaml['INPUT']['SIZE_TRAIN'] = [128, 160]
                uam_yaml['INPUT']['SIZE_TEST'] = [128, 160]

            uam_yaml_path = uam_dir / "config_train.yaml"
            with open(uam_yaml_path, "w") as f:
                yaml.dump(uam_yaml, f)

            run_training(str(uam_yaml_path))

            # === Esperar pesos de UAM
            ckpt_files = list((uam_dir / "model").glob("*.pth"))
            if not ckpt_files:
                print(f" No checkpoint para {cls} {model_type} {pair_name}, saltando...")
                continue
            pretrained_path = str(ckpt_files[0])

            # --- CONFIG TRAIN RIVAS (usando los pesos de UAM) ---
            rivas_train_yaml = copy.deepcopy(BASE_YAML_TEMPLATE)
            rivas_train_yaml['MODEL']['TRANSFORMER_TYPE'] = model_type
            rivas_train_yaml['MODEL']['PRETRAIN_CHOICE'] = 'self'
            rivas_train_yaml['MODEL']['PRETRAIN_PATH'] = pretrained_path
            rivas_train_yaml['DATASETS']['ROOT_DIR'] = paths['RIVAS'].format(cls)
            rivas_train_yaml['DATASETS']['TRAIN'] = ['UAM']
            rivas_train_yaml['DATASETS']['TEST'] = ['UAM_test']
            rivas_train_yaml['SOLVER']['MAX_EPOCHS'] = 1  # Puedes reducir epochs aquí para fine-tuning
            rivas_train_yaml['LOG_ROOT'] = str(rivas_dir / "log")
            rivas_train_yaml['LOG_NAME'] = str(rivas_dir / "model")
            rivas_train_yaml['TB_LOG_ROOT'] = str(rivas_dir / "tb_log")
            rivas_train_yaml['OUTPUT_DIR'] =  class_model_root / "RIVAS"
            if cls == "Rubish":
                rivas_train_yaml['INPUT']['SIZE_TRAIN'] = [160, 224]
                rivas_train_yaml['INPUT']['SIZE_TEST'] = [160, 224]
            elif cls == "Crosswalks":
                rivas_train_yaml['INPUT']['SIZE_TRAIN'] = [320, 96]
                rivas_train_yaml['INPUT']['SIZE_TEST'] = [320, 96]
            elif cls == "Containers":
                rivas_train_yaml['INPUT']['SIZE_TRAIN'] = [128, 160]
                rivas_train_yaml['INPUT']['SIZE_TEST'] = [128, 160]
            rivas_train_yaml_path = rivas_dir / "config_train_rivas.yaml"
            with open(rivas_train_yaml_path, "w") as f:
                yaml.dump(rivas_train_yaml, f)

            run_training(str(rivas_train_yaml_path))

            # === Pesos entrenados en RIVAS
            rivas_ckpt_files = list((rivas_dir / "model").glob("*.pth"))
            if not rivas_ckpt_files:
                print(f" No checkpoint tras fine-tuning RIVAS para {cls} {model_type} {pair_name}, saltando update...")
                continue
            rivas_ckpt_path = str(rivas_ckpt_files[0])

            # --- CONFIG UPDATE ---
            update_yaml = copy.deepcopy(rivas_train_yaml)
            update_yaml['TEST']['WEIGHT'] = rivas_ckpt_path
            update_yaml['TEST']['EVAL'] = True
            update_yaml['TEST']['RE_RANKING'] = True
            update_yaml['DATASETS']['TRAIN'] = ['UAM']
            update_yaml['DATASETS']['TEST'] = ['UAM_test']
            update_yaml['LOG_ROOT'] = str(rivas_dir / "log_test")
            update_yaml['LOG_NAME'] = str(rivas_dir / "test_output")
            update_yaml['TB_LOG_ROOT'] = str(rivas_dir / "tb_log_test")
            update_yaml['OUTPUT_DIR'] =  class_model_root / "RIVAS"
            if cls == "Rubish":
                update_yaml['INPUT']['SIZE_TRAIN'] = [160, 224]
                update_yaml['INPUT']['SIZE_TEST'] = [160, 224]
            elif cls == "Crosswalks":
                update_yaml['INPUT']['SIZE_TRAIN'] = [320, 96]
                update_yaml['INPUT']['SIZE_TEST'] = [320, 96]
            elif cls == "Containers":
                update_yaml['INPUT']['SIZE_TRAIN'] = [128, 160]
                update_yaml['INPUT']['SIZE_TEST'] = [128, 160]
            update_yaml_path = rivas_dir / "config_update.yaml"
            with open(update_yaml_path, "w") as f:
                yaml.dump(update_yaml, f)

            track_path = rivas_dir / "track.txt"
            run_update(str(update_yaml_path), str(track_path))

import numpy as np
for class_name in CLASSES:
    for pair_name in DATASET_PAIRS.keys():
        print(f"\nFusionando Base + Small para {class_name} | {pair_name}")
        model_large = "vit_large_patch16_224_TransReID"
        model_large2 = "vit_large_patch16_224_TransReID"
        
        dir1 = os.path.join(TRANSFER_ROOT, f"{class_name}_{model_large}_{pair_name}", "RIVAS")
        dir2 = os.path.join(TRANSFER_ROOT, f"{class_name}_{model_large2}_{pair_name}", "RIVAS")

        output_dir = os.path.join(TRANSFER_ROOT, f"{class_name}_Combined_{pair_name}", "RIVAS", "RESULTS")
        os.makedirs(output_dir, exist_ok=True)
        
        qf1 = np.load(os.path.join(dir1, "qf.npy"))
        gf1 = np.load(os.path.join(dir1, "gf.npy"))
        qf2 = np.load(os.path.join(dir2, "qf.npy"))
        gf2 = np.load(os.path.join(dir2, "gf.npy"))

        qf1 = l2norm(qf1)
        gf1 = l2norm(gf1)
        qf2 = l2norm(qf2)
        gf2 = l2norm(gf2)
        
        # Fusión por suma
        qf_fused = l2norm((qf1 + qf2) / 2)
        gf_fused = l2norm((gf1 + gf2) / 2)
        
        # Similaridades y re-ranking
        q_g = np.dot(qf_fused, gf_fused.T)
        q_q = np.dot(qf_fused, qf_fused.T)
        g_g = np.dot(gf_fused, gf_fused.T)
        
        from utils.re_ranking import re_ranking
        dist = re_ranking(q_g, q_q, g_g, k1=10, k2=6, lambda_value=0.1)
        indices = np.argsort(dist, axis=1)
        
        # Guardar track
        track_path = os.path.join(output_dir, "track.txt")
        with open(track_path, 'w') as f:
            for row in indices:
                adjusted = [str(i+1) for i in row[:100]]
                f.write(" ".join(adjusted) + "\n")

# === Fusión ponderada final entre base, sr y sr_st ===

for class_name in CLASSES:
    print(f"\nFusionando datasets para {class_name}")

    tracks = [
        os.path.join(TRANSFER_ROOT, f"{class_name}_Combined_base", "RIVAS", "RESULTS", "track.txt"),
        os.path.join(TRANSFER_ROOT, f"{class_name}_Combined_base2", "RIVAS", "RESULTS", "track.txt"),
        os.path.join(TRANSFER_ROOT, f"{class_name}_Combined_base3", "RIVAS", "RESULTS", "track.txt"),
    ]

    output_fused = os.path.join(TRANSFER_ROOT, f"{class_name}_FINAL", "RIVAS", "RESULTS")
    os.makedirs(output_fused, exist_ok=True)

    fusionar_tracks(tracks, os.path.join(output_fused, "track.txt"))

print("\n=== Todo completado ===")

# === Generación de archivo de SUBMISSION para Kaggle ===
print("\n=== Generando submission ===")

# === CONFIGURACIÓN GENERAL ===
clases = ["Containers", "Crosswalks", "Rubish"]
base_dir = Path("/home/mdb/DL_Lab3/TRANSFER_PaT/")
results_dir = base_dir / "FINAL"
global_query_xml = Path("/home/mdb/DL_Lab3/Kaggle_Dataset/query_label.xml")

# === SALIDAS ===
names_file = results_dir / "Results_RIVAS_ALL_names.txt"
indices_file = results_dir / "Results_RIVAS_ALL_indices.txt"
submission_file = results_dir / "RESULT_SUBMISSION.txt"
output_csv_path = results_dir / "kaggle_submission_converted_1based.csv"

# === FUNCIONES AUXILIARES ===
def parse_items(xml_path):
    tree = ET.parse(xml_path)
    return [item.attrib["imageName"] for item in tree.getroot().findall("Item")]

def extract_number(filename):
    return str(int(filename.replace(".jpg", "").lstrip("0") or "0"))

# === PASO 1: GENERAR NOMBRES POR CLASE ===
all_entries = []

for cls in clases:
    dataset_dir = Path("/home/mdb/DL_Lab3/RIVAS_DATASET/DATASET")
    query_xml = dataset_dir / cls / "query_label.xml"
    test_xml = dataset_dir / cls / "test_label.xml"
    track_file = base_dir / f"{cls}_FINAL" / "RIVAS" / "RESULTS" / "track.txt"

    query_names = parse_items(query_xml)
    gallery_names = parse_items(test_xml)

    with open(track_file, "r") as f:
        lines = [line.strip().split() for line in f.readlines()]

    assert len(query_names) == len(lines), f"⚠️ Mismatch en {cls}: {len(query_names)} queries vs {len(lines)} predicciones"

    for qname, pred_indices in zip(query_names, lines):
        gallery_preds = []
        for idx in pred_indices:
            try:
                g_idx = int(idx) - 1  # ¡Recordar que son 1-based!
                if 0 <= g_idx < len(gallery_names):
                    gallery_preds.append(gallery_names[g_idx])
                else:
                    gallery_preds.append("unknown.jpg")
            except:
                gallery_preds.append("error.jpg")
        all_entries.append((qname, gallery_preds))

# === PASO 2: ORDEN GLOBAL Y GUARDAR NOMBRES ===
global_query_names = parse_items(global_query_xml)
query_to_predictions = {q: preds for q, preds in all_entries}

final_lines = []
missing = []

for qname in global_query_names:
    preds = query_to_predictions.get(qname)
    if preds:
        final_lines.append(" ".join([qname] + preds))
    else:
        final_lines.append(qname + " " + "missing.jpg "*100)
        missing.append(qname)

results_dir.mkdir(parents=True, exist_ok=True)
with open(names_file, "w") as f:
    f.write("\n".join(final_lines))

print(f"Guardado NOMBRES en: {names_file}")

# === PASO 3: CONVERTIR A ÍNDICES NUMÉRICOS ===
with open(names_file, "r") as fin:
    lines = fin.readlines()

with open(indices_file, "w") as fout:
    for line in lines:
        parts = line.strip().split()
        numbers = [extract_number(name) for name in parts]
        fout.write(" ".join(numbers) + "\n")

print(f"Guardado ÍNDICES en: {indices_file}")

# === PASO 4: ELIMINAR ÍNDICE DE QUERY PARA SUBMISSION FINAL ===
with open(indices_file, "r") as fin, open(submission_file, "w") as fout:
    for line in fin:
        parts = line.strip().split()
        fout.write(" ".join(parts[1:]) + "\n")  # Skip query name

print(f"Guardado RESULT_SUBMISSION en: {submission_file}")

# === PASO 5: CREAR CSV PARA KAGGLE SUBMISSION ===
with open(submission_file, "r") as f:
    prediction_lines = [line.strip().split() for line in f.readlines()]

num_queries = len(global_query_names)
assert len(prediction_lines) == num_queries, f"⚠️ Esperados {num_queries} queries, pero hay {len(prediction_lines)}"

converted_lines = []
all_indices = []

for i, preds in enumerate(prediction_lines):
    image_name = "{:06d}.jpg".format(i + 1)  # 1-based naming
    valid_preds = [str(int(x)) for x in preds if x.isdigit()]
    converted_lines.append([image_name, " ".join(valid_preds)])
    all_indices.extend(map(int, valid_preds))

with open(output_csv_path, "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerow(["imageName", "Corresponding Indexes"])
    writer.writerows(converted_lines)

min_idx = min(all_indices)
max_idx = max(all_indices)

print(f"CSV convertido para Kaggle guardado en: {output_csv_path}")
print(f"Índices predichos: mínimo={min_idx} | máximo={max_idx}")

# === AVISO FINAL ===
if missing:
    print(f"⚠️ WARNING: {len(missing)} queries faltantes (ejemplo: {missing[:5]})")

print("\n=== PROCESO COMPLETADO ===")
